# CSCI 12033 Computer Architecture and Design - Lecture 02

## Contents

1.  Introduction to Computer Architecture
2.  History of Computers
3.  Architectural Classification
4.  Flynn's Taxonomy of Computer Architecture
5.  Types of Flynn's Taxonomy

## 1. Introduction to Computer Architecture

Computer architecture refers to the design and organization of computer systems. It includes the structure of components and how they interact with each other.

Its scope encompasses both hardware and software aspects. It focuses on the functional behavior of the system as seen by the user or programmer.

Architecture determines a computer's performance, efficiency, and capabilities. It also influences power consumption, cost, and scalability.

*(Note: A link to a video is included in the original source: `https://youtu.be/t6_yhVTDfUE?feature=shared`)*

## 2. History of Computers

### The Analytical Engine (Charles Babbage, 19th Century)

The Analytical Engine was a groundbreaking mechanical computing device conceptualized by Charles Babbage in the 19th century. It represented a significant leap forward in computing technology and inspired future developments.

**Purpose and Design:**

*   Designed as a versatile, general-purpose machine to improve upon Babbage's earlier invention, the Difference Engine.
*   Introduced the concept of a stored program, where instructions and data were stored on punched cards, foreshadowing modern computer architectures.

**Key Components:**

*   **CPU (Mill):** Performed arithmetic and logical operations.
*   **Memory Unit (Store):** Stored data and intermediate results.
*   **Input/Output Devices:** Used punched cards for input and output.
*   **Control Unit:** Managed the execution of instructions.

**Advanced Features:**

*   Supported loops and conditional branching, enabling complex calculations and decision-making.
*   Capable of handling complex mathematical functions and operations.
*   Designed with modularity, allowing for flexibility and scalability.

**Innovative Concepts:**

*   Pioneered the idea of programmability, making it one of the earliest designs for a general-purpose computer.
*   Introduced the concept of sequential control, where operations were executed in a specific order.

**Legacy and Impact:**

*   Although never fully constructed during Babbage's lifetime, the detailed plans and designs of the Analytical Engine had a profound influence on computing.
*   Babbage's visionary ideas, including the stored program concept, laid the foundation for modern computer architecture.
*   Inspired later pioneers like Ada Lovelace, who wrote the first algorithm intended for the Analytical Engine, earning her recognition as the first computer programmer.

### Ada Lovelace

Ada Lovelace, a mathematician, collaborated closely with Charles Babbage and recognized the broader potential of the Analytical Engine. She saw the machine as more than just a tool for numerical calculations, envisioning its use for general-purpose computing.

**Key Contributions:**

*   Published extensive notes on the Analytical Engine, explaining its functionality and potential applications.
*   Included an algorithm for calculating Bernoulli numbers, which is considered the first computer program in history.
*   Her work demonstrated the machine's ability to perform complex tasks beyond arithmetic, such as processing symbols and creating music or art.

**Visionary Insights:**

*   Highlighted the concept of programmability, emphasizing that the Analytical Engine could be instructed to perform a wide range of tasks.
*   Recognized the potential for conditional branching and loops, enabling the machine to make decisions and repeat operations.
*   Described the idea of a general-purpose computing device, foreshadowing modern computers.

**Legacy and Impact:**

*   Ada Lovelace is often regarded as the first computer programmer due to her groundbreaking work on the Analytical Engine.
*   Her insights laid the foundation for the software industry and the development of algorithmic thinking.
*   Demonstrated the importance of interdisciplinary collaboration between mathematics and engineering in advancing technology.

### Turing Machine (Alan Turing, 1930s)

Alan Turing, a British mathematician and computer scientist, revolutionized computation with his theoretical concept of the Turing Machine. Proposed in the 1930s, the Turing Machine provided a mathematical model for simulating any computer algorithm.

**Key Concepts:**

*   **Universal Computing Machine:** Introduced the idea of a machine capable of emulating the behavior of any other machine.
*   **Algorithmic Processing:** Established the theoretical basis for computation, defining what it means to solve a problem algorithmically.
*   **Tape-Based Model:** Consisted of an infinite tape (memory), a read/write head, and a set of states to perform operations.

**Significance:**

*   Laid the foundation for modern computer science and the theory of computation.
*   Demonstrated the limits of computation, including the concept of computability and undecidable problems.
*   Influenced the design of programming languages, compilers, and modern computers.

**Impact on Computer Science:**

*   **Algorithms:** Provided a framework for understanding and analyzing algorithms.
*   **Complexity Theory:** Inspired the study of computational complexity and efficiency.
*   **Artificial Intelligence:** Influenced early ideas about machine intelligence and problem-solving.

### The Von Neumann Architecture (John von Neumann, Late 1940s)

The von Neumann architecture, developed in the late 1940s by John von Neumann, revolutionized computer design and became the foundation for modern computers. It introduced a stored-program concept, where both instructions and data are stored in the same memory system.

**Key Components:**

*   **Central Processing Unit (CPU):**
    *   Acts as the "brain" of the computer.
    *   Performs arithmetic, logical, control, and input/output operations.
*   **Memory:**
    *   A unified memory system that stores both instructions and data.
    *   Allows the CPU to access and process information efficiently.
*   **Input/Output Devices:**
    *   Enable interaction with the external world (e.g., keyboards, displays, printers, storage devices).
*   **Control Unit:**
    *   Manages and coordinates the flow of instructions and data within the system.
    *   Ensures synchronized operation of all components.

**Key Features:**

*   **Stored-Program Concept:** Instructions and data are stored in the same memory, enabling programmability and flexibility.
*   **Sequential Execution:** Instructions are executed one at a time in a sequential manner.
*   **Fetch-Decode-Execute Cycle:** The CPU fetches instructions from memory, decodes them, and executes them in a continuous cycle.

*(Diagram Description: A diagram illustrating the components of a Von Neumann architecture. It shows a Central Processing Unit (CPU) containing a Control Unit, ALU, and Registers. The CPU is connected via a Bus to Main Memory, Secondary Memory (Storage), and Input/Output Devices (Keyboard, Mouse, Display, Printer). Arrows indicate data flow between components via the bus.)*

### Concept of Stored-Program Computers

The stored-program concept is the most significant aspect of the von Neumann architecture. It revolutionized computing by allowing instructions and data to be stored in the same memory system.

**Key Features:**

*   **Unified Memory:** Both instructions (programs) and data are stored in the same memory. Enables the computer to fetch, decode, and execute instructions directly from memory.
*   **Programmability:** Programs can be modified, stored, and executed without requiring physical changes to the machine. Treats programs as data, allowing for dynamic and flexible computing.

Earlier computers used separate memories for instructions and data, which required manual intervention (e.g., rewiring or reprogramming) to change instructions. This limited flexibility and efficiency.

Von Neumann Architecture's unified memory system simplifies design and operation. It enables automatic execution of programs stored in memory.

### Revolutionizing Computer Design and Programming

The stored-program concept in the von Neumann architecture transformed computer design and programming. It enabled flexible and efficient programming by treating instructions as data.

**Key Features:**

*   **Instructions Stored in Memory:** Program instructions could be stored, retrieved, and executed sequentially from the same memory as data.
*   **Dynamic Execution:** Programs could be modified and executed without requiring physical changes to the hardware.

### Impact on Programming

*   **Software-Driven Behavior:** Programs could control the machine's operations, making computers general-purpose and versatile.
*   **Ease of Development:** Simplified the process of writing, testing, and modifying programs.
*   **Foundation for Modern Computing:** Paved the way for programming languages, operating systems, and software applications.

### Impact and Legacy

The von Neumann architecture became the foundation for modern computer design and is still widely used today.

*   It enabled the development of high-level programming languages and sophisticated software applications by separating the program's logic from the underlying hardware.
*   The concept of stored-program computers transformed computers from specialized machines designed for specific tasks to general-purpose computing devices.
*   The von Neumann architecture's impact on computer design and programming has been instrumental in the rapid advancement and widespread adoption of computers in various domains, fueling the digital revolution.

## 3. Architectural Classification

Computer architecture can be classified into different layers or perspectives:

### Instruction Set Architecture (ISA)

Instruction Set Architecture (ISA) defines the instructions and operations a computer can execute, along with the programming model provided to software. Provides a level of abstraction, allowing programmers to write software without needing to understand the underlying hardware details.

**Key Components:**

*   **Instruction Set:** The collection of instructions the CPU can understand and execute (e.g., arithmetic, logic, control, and data movement).
*   **Instruction Formats:** Specifies how instructions are encoded and structured in binary form.
*   **Addressing Modes:** Defines how operands are accessed or addressed during instruction execution (e.g., direct, indirect, immediate).
*   **Hardware Interface:** Describes the registers, memory organization, and data types supported by the hardware.

ISA acts as a bridge between hardware and software, providing a standardized interface for programming. It enables software portability by allowing programs to run on different hardware implementations of the same ISA.

**Examples:** x86, ARM, MIPS

### Microarchitecture

Microarchitecture refers to the internal design of a processor, including its data path, control unit, and memory hierarchy.

**Key Components:**

*   **Data Path:** Handles data flow and arithmetic/logic operations.
*   **Control Unit:** Manages instruction execution and resource coordination.
*   **Memory Hierarchy:** Includes registers, caches, and main memory for efficient data access.

Microarchitecture determines how the processor executes instructions, manages data transfers, and optimizes resource usage. It implements techniques like instruction pipelining, branch prediction, and parallel processing to enhance performance.

Microarchitecture directly impacts the performance, efficiency, and power consumption of the processor and bridges the gap between Instruction Set Architecture (ISA) and hardware implementation.

**Examples:** Intel Core, AMD Ryzen

### System Architecture

System Architecture focuses on the overall structure and interconnections of a computer system's components. Ensures scalability, reliability, and performance of the entire system and balances hardware and software interactions for optimal functionality.

**Key Components:**

*   **Memory Subsystems:** Design of RAM, caches, and storage hierarchies.
*   **Input/Output Devices:** Management of peripherals like keyboards, displays, and storage.
*   **Buses and Communication:** Interconnects for data transfer between components.

**Key Features:** Interrupt Handling, Power Management, Multiprocessing, and Virtualization.

**Examples:** Server Architectures, Embedded Systems

### Importance of Architectural Classification

Classification helps in understanding and analyzing different aspects of computer systems:

*   **Instruction Set Architecture (ISA):** Provides a standardized programming model and interface for software development. Ensures software compatibility across different hardware implementations.
*   **Microarchitecture:** Impacts processor performance, power consumption, and efficiency. Involves optimizations like pipelining, branch prediction, and caching.
*   **System Architecture:** Ensures effective communication and coordination among system components. Facilitates smooth data flow and system operation. Includes design of memory subsystems, I/O devices, and communication infrastructure.

**Benefits of Classification:**

*   Enables informed decisions in hardware design, software development, and system integration.
*   Supports scalability, reliability, and performance optimization.
*   Aids researchers, engineers, and programmers in innovating and optimizing computer systems.

### KNOWLEDGE CHECK

How do ISA, microarchitecture, and system architecture work together to execute a program efficiently in a computer system?

### KNOWLEDGE CHECK EXPLAINED

*   The **ISA** defines the set of instructions the processor can execute and provides a standardized interface for software, hence it defines what the program can do.
*   The **microarchitecture** implements the ISA by designing the internal structure of the processor, hence it determines how efficiently the processor executes those instructions.
*   The **system architecture** organizes the overall system to ensure smooth data flow and communication. Hence it ensures that all components (CPU, memory, I/O) work together seamlessly to execute the program.

## 4. Flynn's Taxonomy of Computer Architecture

Proposed by Michael Flynn in 1966, Flynn's Taxonomy classifies computer architectures based on concurrent instruction streams (I) and data streams (D).

*   Provides a framework for understanding and analyzing parallel computing architectures.
*   Helps in designing systems for performance optimization and scalability.

*(Image Description: A photograph of Michael Flynn.)*

## 5. Types of Flynn's Taxonomy

In Flynn's classification, either the instruction or data streams can be single or multiple. Computer architecture can be classified into the following four distinct categories:

1.  **SISD (Single Instruction, Single Data):** One instruction stream and one data stream (e.g., traditional single-core processors).
2.  **SIMD (Single Instruction, Multiple Data):** One instruction stream and multiple data streams (e.g., GPUs, vector processors).
3.  **MISD (Multiple Instruction, Single Data):** Multiple instruction streams and one data stream (rarely used in practice).
4.  **MIMD (Multiple Instruction, Multiple Data):** Multiple instruction streams and multiple data streams (e.g., multi-core processors, distributed systems).

*(Diagram Description: A 2x2 matrix diagram showing Flynn's Classification. The horizontal axis is "Data Stream" (Single, Multiple), and the vertical axis is "Instruction Stream" (Single, Multiple). The four quadrants are: Top-Left: SISD (Uniprocessors), Top-Right: SIMD (Vector Processors, Parallel Processing), Bottom-Left: MISD (May be Pipelined Computers), Bottom-Right: MIMD (Multi-Computers, Multi-Processors).)*

### SISD (Single Instruction, Single Data)

Executes one instruction on one data item at a time. Operates sequentially, performing one operation at a time.

**Key Characteristics:**

*   **Single Processor:** Fetches data from a single memory address and executes one instruction at a time.
*   **Sequential Execution:** No parallel processing or data-level parallelism.

**Applications:**

*   Suitable for simple tasks and low-power systems.
*   All single-processor systems fall under SISD.

**Examples:** Von Neumann Architecture, Simple Microcontrollers, Early single-processor systems.

*(Diagram Description: A block diagram for SISD. It shows an I/O block connected to a CU/PE (Control Unit / Processing Element) block. The CU/PE block is connected to an M (Memory) block. An "Instruction Stream" arrow points from M to CU/PE. A "Data Stream" arrow goes bi-directionally between CU/PE and M. The I/O block also connects to the CU/PE.)*

Instructions are decoded by the Control Unit and then the Control Unit sends the instructions to the processing units for execution. Data Stream flows between the processors and memory bi-directionally.

**Advantages of SISD:**

*   **Simplicity:** Easy to design and implement due to its straightforward, sequential operation. No need for complex mechanisms to handle parallel execution or synchronization.
*   **Low Cost:** Requires fewer hardware resources compared to parallel architectures. Suitable for low-power and low-cost devices like microcontrollers.
*   **Predictable Performance:** Execution is deterministic, making it easier to predict and analyze performance.
*   **Ease of Programming:** Programs are written for sequential execution, simplifying software development. No need to manage parallel threads or data synchronization.
*   **Reliability:** Fewer components and simpler design reduce the likelihood of hardware failures.

**Disadvantages of SISD:**

*   **Limited Performance:** Cannot exploit parallelism, making it slow for complex computations or large datasets. Bottlenecked by sequential execution of instructions.
*   **Scalability Issues:** Not suitable for modern applications requiring high-speed processing or multitasking.
*   **Inefficient for Modern Workloads:** Struggles with tasks like graphics rendering, scientific simulations, or big data processing, which benefit from parallel execution.
*   **Resource Underutilization:** Only one instruction and one data stream are processed at a time, leaving hardware resources underutilized.
*   **Outdated for High-Performance Computing:** Modern applications increasingly rely on parallel architectures (e.g., SIMD, MIMD) for better performance and efficiency.

### SIMD (Single Instruction, Multiple Data)

Executes one instruction on multiple pieces of data simultaneously.

**Execution Modes:**

*   **Sequential:** Uses pipelining to improve efficiency.
*   **Parallel:** Utilizes multiple processors or cores for simultaneous execution.

**Applications:**

*   Modern GPUs: Use SIMD for graphics rendering and parallel computations.
*   Vector Processors: Handle large datasets efficiently (e.g., scientific simulations).
*   Array Processors: Perform operations on arrays or matrices in parallel.

**Examples:** Intel SSE/AVX, NVIDIA CUDA, Graphics Processing, Scientific Computing.

*(Diagram Description: A block diagram for SIMD. It shows an "Instruction pool" feeding a "Vector unit". A "Data pool" feeds multiple Processing Units (PU). The Vector Unit provides a single instruction stream to all PUs simultaneously. The PUs operate on data from the Data Pool. An image also shows a stylized representation with a data pool and multiple PUs receiving instructions from an instruction pool via a vector unit.)*

All processors receive the same instruction from the control unit but operate on different items of data. The shared memory unit must contain multiple modules so that it can communicate with all the processors simultaneously.

*(Diagram Description: A block diagram for SIMD showing a Data Bus connected to a Control Unit and a Memory. The Control Unit provides an Instruction Stream to multiple Processing Units (P). The Processors (P) are connected to an Alignment Network via Data Stream lines. The Alignment Network is connected to multiple Memory Modules (M), which are connected to the Memory block. The Data Bus also connects to the Alignment Network/Processors via the Memory modules.)*

*   A multiprocessor machine where all CPUs execute the same instruction but operate on different data streams.
*   Ideal for scientific computing involving vector and matrix operations.
*   Commonly used in array processing machines and vector processors.
*   Data elements are divided into multiple sets (N sets for N processing elements).
*   Each Processing Element (PE) processes one data set simultaneously.
*   Cray's Vector Processing Machines: Dominant representatives of SIMD systems.
*   Modern GPUs: Use SIMD for parallel computations in graphics and machine learning.

*(Diagram Description: A diagram showing the execution time for a vector operation across three processors P1, P2, Pn. Each processor shows sequential steps: 'prev instruct', 'load A(i)', 'load B(i)', 'C(i)=A(i)*B(i)', 'store C(i)', 'next instruct'. The bars next to the steps indicate the relative time, showing the operations happening in parallel across P1, P2, and Pn for their respective data elements A(1), B(1); A(2), B(2); ... A(n), B(n).)*

**Working of SIMD:**

*   The SIMD model of parallel computing consists of two parts: a front-end computer of the usual von Neumann style, and a processor array.
*   The processor array is a set of identical synchronized processing elements capable of simultaneously performing the same operation on different data.
*   Each processor in the array has a small amount of local memory where the distributed data resides while it is being processed in parallel.
*   The processor array is connected to the memory bus of the front end so that the front end can randomly access the local processor memories as if it were another memory.
*   The front end can issue special commands that cause parts of the memory to be operated on simultaneously or cause data to move around in the memory.
*   The application program is executed by the front end in the usual serial way, but issues commands to the processor array to carry out SIMD operations in parallel.

**Advantages of SIMD:**

*   **High Performance:** Executes one instruction on multiple data elements simultaneously, making it ideal for data-parallel tasks.
*   **Significantly speeds up:** Operations like vector/matrix computations, image processing, and scientific simulations.
*   **Efficiency:** Reduces instruction fetch overhead by applying a single instruction to multiple data points. Optimizes resource utilization by processing multiple data streams in parallel.
*   **Scalability:** Scales well for tasks involving large datasets or repetitive operations. Suitable for modern GPUs and vector processors.
*   **Energy Efficiency:** Performs more computations per clock cycle, reducing energy consumption for parallelizable tasks.
*   **Wide Applications:** Used in graphics rendering, machine learning, signal processing, and scientific computing.

**Disadvantages of SIMD:**

*   **Limited Flexibility:** Only effective for tasks with data-level parallelism. Struggles with irregular or branch-heavy algorithms.
*   **Complex Programming:** Requires specialized programming techniques (e.g., vectorization) to exploit parallelism. Not all algorithms can be easily adapted for SIMD execution.
*   **Hardware Dependency:** Performance gains depend on the availability of SIMD-capable hardware (e.g., GPUs, vector processors). Not all processors support SIMD instructions.
*   **Data Alignment Issues:** Requires data to be aligned in memory for efficient processing, which can complicate memory management.
*   **Underutilization for Non-Parallel Tasks:** Inefficient for sequential or scalar operations, leading to underutilization of resources.

### MISD (Multiple Instruction, Single Data)

Stands for Multiple Instruction and Single Data stream. Multiple Processing Units with each unit processing the same data but performing different operations. Independent Instruction Streams where each processor follows its own set of instructions.

**Key Concept:** Multiple processors work on the same data set but execute different instructions simultaneously.

**Structure:**

*   A single data stream flows through a linear array of processors.
*   Each processor operates on the data independently using its own instruction stream.

**Examples:** Parallel computing theory. (Rarely used in practice for general-purpose computing).

*(Diagram Description: A diagram showing the execution time for operations on a single data element A(1) across three processors P1, P2, Pn. Each processor shows sequential steps: 'prev instruct', 'load A(1)', 'C(1)=A(1)*[different operation]', 'store C(1)', 'next instruct'. The middle step 'C(i)=A(1)*[different operation]' varies for each processor (e.g., P1 has C(1)=A(1)\*1, P2 has C(2)=A(1)\*2, Pn has C(n)=A(1)\*n). This illustrates multiple instructions operating on the same data.)*

**Use Cases:**

*   **Fault-Tolerant Systems:** In certain critical applications, redundancy is employed to ensure system reliability. In a fault-tolerant system based on MISD architecture, multiple instructions can be executed simultaneously on the same data set to detect and correct errors. This can be useful in aerospace systems, nuclear power plants, or other safety-critical environments.
*   **Encryption and Decryption:** In cryptographic systems, multiple encryption or decryption algorithms can be applied concurrently to the same data set using the MISD model. This could provide enhanced security by utilizing diverse algorithms to process the data simultaneously.
*   **Image Processing:** In certain image processing tasks, such as feature extraction or filtering, different algorithms can be applied in parallel to the same image data using MISD architecture. Each instruction can extract different features or apply different filters simultaneously, leading to efficient processing of the image data.

**Advantages of MISD:**

*   **Theoretical Interest:** Provides a unique perspective on parallel computing and instruction-level parallelism. Useful for academic research and exploring unconventional architectures.
*   **Specialized Use Cases:** Could potentially be applied in fault-tolerant systems where multiple processors verify or process the same data differently.
*   **Conceptual Framework:** Helps in understanding the limits and possibilities of parallel processing architectures.

**Disadvantages of MISD:**

*   **No Practical Implementations:** No real-world systems have been built using MISD architecture due to its impracticality.
*   **Limited Applicability:** Not suitable for most real-world computing tasks, which require multiple data streams or shared data processing.
*   **Complexity:** Managing multiple instruction streams on a single data stream is complex and inefficient.
*   **Resource Underutilization:** Multiple processors working on the same data stream may lead to redundant computations and inefficient resource usage.
*   **Lack of Scalability:** Difficult to scale for large-scale or diverse computing tasks.

### MIMD (Multiple Instruction, Multiple Data)

A parallel computing architecture where multiple processors execute different instructions on different data sets simultaneously.

**Key Features:**

*   **Independent Processors:** Each processor operates independently, executing its own program on its local data.
*   **Concurrent Execution:** Enables parallel processing of multiple instructions and data streams.

**Memory Architecture:**

*   **Distributed Memory:** Each processor has its own local memory and accesses data independently.
*   **Communication Mechanisms:** Processors communicate via message passing or shared memory for coordination and data exchange.

**Operation:**

*   Multiple autonomous processors work on different pieces of data, either independently or within a shared memory space.
*   Allows several different instructions to be executed simultaneously on different data streams.

**Use Cases:**

*   High-Performance Computing (HPC): Scientific simulations, weather modeling, etc.
*   Distributed Systems: Cloud computing, cluster computing.
*   Multicore Processors: Modern CPUs and GPUs.

**Examples:**

*   Supercomputers: Use MIMD architecture for large-scale parallel processing.
*   Multicore CPUs: Execute multiple threads or processes simultaneously.

*(Diagram Description: A block diagram for MIMD. It shows an "Instruction pool" connected to multiple Processing Units (PU), with separate instruction streams going to different PUs. A "Data pool" is also connected to the PUs, with data streams flowing to different PUs. This illustrates multiple independent instruction streams and multiple data streams feeding separate processors.)*

**Advantages of MIMD:**

*   **High Performance:** Enables parallel execution of multiple instructions on multiple data streams, significantly improving performance for complex tasks.
*   **Flexibility:** Supports a wide range of applications, from scientific simulations to real-time processing. Each processor can execute different programs independently, making it versatile.
*   **Scalability:** Can scale to a large number of processors, making it suitable for high-performance computing (HPC) and distributed systems.
*   **Efficient Resource Utilization:** Processors work independently, reducing idle time and maximizing resource usage.
*   **Distributed Memory Architecture:** Each processor has its own local memory, reducing memory contention and improving efficiency. Supports shared memory or message passing for inter-processor communication.
*   **Fault Tolerance:** If one processor fails, others can continue operating, enhancing system reliability.

**Disadvantages of MIMD:**

*   **Complexity:** Designing and programming MIMD systems is complex due to the need for synchronization and communication between processors.
*   **Communication Overhead:** Inter-processor communication (e.g., message passing) can introduce latency and reduce performance.
*   **Cost:** Requires expensive hardware and infrastructure to support multiple processors and memory systems.
*   **Programming Challenges:** Writing parallel programs for MIMD systems requires expertise in parallel programming models (e.g., MPI, OpenMP). Debugging and optimizing parallel code can be difficult.
*   **Load Balancing:** Ensuring an even distribution of work across processors can be challenging, leading to inefficient resource utilization.
*   **Memory Management:** In shared memory systems, managing memory access and avoiding race conditions can be complex.

## What Did We Talk About?

**Computer Architecture:**

*   **ISA:** Defines instructions and software-hardware interface.
*   **Microarchitecture:** Internal processor design (e.g., pipelines, caches).
*   **System Architecture:** Overall system structure (memory, I/O, buses).

**Flynn's Taxonomy:**

*   Classifies architectures by instruction streams (I) and data streams (D).
*   Categories: SISD, SIMD, MISD, MIMD.
*   Guides design of parallel and high-performance systems.

## 2 - MINUTE PAPER

Describe the takeaway from today's lesson in your own words.

*(Image Description: A silhouette of a person scratching their head with a question mark above their head, indicating thinking or confusion.)*

## THANK YOU!

05.03.2025